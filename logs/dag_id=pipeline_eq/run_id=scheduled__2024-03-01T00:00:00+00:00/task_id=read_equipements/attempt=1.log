[2024-04-22T16:56:03.998+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T16:56:04.004+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T16:56:04.005+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T16:56:04.018+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-22T16:56:04.022+0000] {standard_task_runner.py:60} INFO - Started process 165075 to run task
[2024-04-22T16:56:04.024+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpqax22bky']
[2024-04-22T16:56:04.025+0000] {standard_task_runner.py:88} INFO - Job 7: Subtask read_equipements
[2024-04-22T16:56:04.070+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T16:56:04.199+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-22T16:56:04.204+0000] {pipeline_eq.py:47} INFO - start to get objects in minio
[2024-04-22T16:56:04.233+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:315 AirflowContextDeprecationWarning: Accessing 'prev_execution_date_success' from the template is deprecated and will be removed in a future version. Please use 'prev_data_interval_start_success' instead.
[2024-04-22T16:56:04.235+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 48, in read_equipements
    intervals =[context['prev_execution_date_success'].date, context['execution_date'].date]
AttributeError: 'NoneType' object has no attribute 'date'
[2024-04-22T16:56:04.258+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240422T165603, end_date=20240422T165604
[2024-04-22T16:56:04.296+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 7 for task read_equipements ('NoneType' object has no attribute 'date'; 165075)
[2024-04-22T16:56:04.317+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-22T16:56:04.359+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T17:09:56.453+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T17:09:56.461+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T17:09:56.461+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T17:09:56.474+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-22T17:09:56.478+0000] {standard_task_runner.py:60} INFO - Started process 179042 to run task
[2024-04-22T17:09:56.481+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '59', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpadbq1e95']
[2024-04-22T17:09:56.482+0000] {standard_task_runner.py:88} INFO - Job 59: Subtask read_equipements
[2024-04-22T17:09:56.521+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T17:09:56.574+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-22T17:09:56.575+0000] {pipeline_eq.py:48} INFO - start to get objects in minio
[2024-04-22T17:09:58.945+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:315 AirflowContextDeprecationWarning: Accessing 'prev_execution_date_success' from the template is deprecated and will be removed in a future version. Please use 'prev_data_interval_start_success' instead.
[2024-04-22T17:09:58.946+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:315 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T17:09:58.947+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 64, in read_equipements
    raise ValueError(f"No files found with good extensions {extensions}")
ValueError: No files found with good extensions ['.json']
[2024-04-22T17:09:58.951+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240422T170956, end_date=20240422T170958
[2024-04-22T17:09:58.963+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 59 for task read_equipements (No files found with good extensions ['.json']; 179042)
[2024-04-22T17:09:58.980+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-22T17:09:58.995+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T17:12:35.859+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T17:12:35.870+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T17:12:35.870+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T17:12:35.886+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-22T17:12:35.890+0000] {standard_task_runner.py:60} INFO - Started process 181932 to run task
[2024-04-22T17:12:35.893+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '62', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpgahde072']
[2024-04-22T17:12:35.894+0000] {standard_task_runner.py:88} INFO - Job 62: Subtask read_equipements
[2024-04-22T17:12:35.938+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T17:12:35.992+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-22T17:12:35.993+0000] {pipeline_eq.py:48} INFO - start to get objects in minio
[2024-04-22T17:12:38.116+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:315 AirflowContextDeprecationWarning: Accessing 'prev_execution_date_success' from the template is deprecated and will be removed in a future version. Please use 'prev_data_interval_start_success' instead.
[2024-04-22T17:12:38.116+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 60, in read_equipements
    good_objects = [obj.object_name for obj in objects if obj.object_name.lower().endswith(tuple(extensions)) and obj.last_modified.date in [intervals[0].date, intervals[1].date] ]
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 60, in <listcomp>
    good_objects = [obj.object_name for obj in objects if obj.object_name.lower().endswith(tuple(extensions)) and obj.last_modified.date in [intervals[0].date, intervals[1].date] ]
AttributeError: 'NoneType' object has no attribute 'date'
[2024-04-22T17:12:38.120+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240422T171235, end_date=20240422T171238
[2024-04-22T17:12:38.129+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 62 for task read_equipements ('NoneType' object has no attribute 'date'; 181932)
[2024-04-22T17:12:38.152+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-22T17:12:38.165+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:16:51.378+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:16:51.384+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:16:51.384+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:16:51.396+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-22T18:16:51.398+0000] {standard_task_runner.py:60} INFO - Started process 234714 to run task
[2024-04-22T18:16:51.400+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '95', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpp3os8gd4']
[2024-04-22T18:16:51.401+0000] {standard_task_runner.py:88} INFO - Job 95: Subtask read_equipements
[2024-04-22T18:16:51.428+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:16:51.465+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:16:51.488+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-22T18:16:51.489+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:16:51.489+0000] {logging_mixin.py:188} INFO - intervals is ['None', '2024-03-01 00:00:00+00:00']
[2024-04-22T18:16:53.677+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:16:53.784+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240422T181651, end_date=20240422T181653
[2024-04-22T18:16:53.820+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:16:53.848+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-01T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:16:53.855+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:25:11.087+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:25:11.093+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:25:11.094+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:25:11.110+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-22T18:25:11.115+0000] {standard_task_runner.py:60} INFO - Started process 245155 to run task
[2024-04-22T18:25:11.117+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '148', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpa_jt47i7']
[2024-04-22T18:25:11.118+0000] {standard_task_runner.py:88} INFO - Job 148: Subtask read_equipements
[2024-04-22T18:25:11.147+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:25:11.182+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:25:11.204+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-22T18:25:11.205+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:25:11.205+0000] {logging_mixin.py:188} INFO - intervals is ['None', '2024-03-01 00:00:00+00:00']
[2024-04-22T18:25:13.432+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:25:13.548+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240422T182511, end_date=20240422T182513
[2024-04-22T18:25:13.576+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:25:13.601+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-01T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:25:13.607+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:27:05.373+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:27:05.381+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:27:05.382+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:27:05.397+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-22T18:27:05.402+0000] {standard_task_runner.py:60} INFO - Started process 247505 to run task
[2024-04-22T18:27:05.406+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '160', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmprccov2_0']
[2024-04-22T18:27:05.407+0000] {standard_task_runner.py:88} INFO - Job 160: Subtask read_equipements
[2024-04-22T18:27:05.443+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:27:05.484+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:27:05.508+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-22T18:27:05.509+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:27:05.510+0000] {logging_mixin.py:188} INFO - intervals is ['None', '2024-03-01 00:00:00+00:00']
[2024-04-22T18:27:07.822+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:27:07.944+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240422T182705, end_date=20240422T182707
[2024-04-22T18:27:07.984+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:27:08.005+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-01T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:27:08.012+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:31:29.434+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:31:29.444+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:31:29.444+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:31:29.464+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-22T18:31:29.496+0000] {standard_task_runner.py:60} INFO - Started process 252526 to run task
[2024-04-22T18:31:29.500+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '175', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmprhtm98my']
[2024-04-22T18:31:29.501+0000] {standard_task_runner.py:88} INFO - Job 175: Subtask read_equipements
[2024-04-22T18:31:29.536+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:31:29.574+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:31:29.600+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-22T18:31:29.601+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:31:29.601+0000] {logging_mixin.py:188} INFO - intervals is ['None', '2024-03-01 00:00:00+00:00']
[2024-04-22T18:31:32.319+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:31:32.445+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240422T183129, end_date=20240422T183132
[2024-04-22T18:31:32.479+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:31:32.506+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-01T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:31:32.514+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:33:01.373+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:33:01.379+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:33:01.379+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:33:01.392+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-22T18:33:01.395+0000] {standard_task_runner.py:60} INFO - Started process 254349 to run task
[2024-04-22T18:33:01.397+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '179', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmps9mjtxt6']
[2024-04-22T18:33:01.398+0000] {standard_task_runner.py:88} INFO - Job 179: Subtask read_equipements
[2024-04-22T18:33:01.428+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:33:01.463+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:33:01.488+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-22T18:33:01.488+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:33:01.489+0000] {logging_mixin.py:188} INFO - intervals is ['None', '2024-03-01 00:00:00+00:00']
[2024-04-22T18:33:03.858+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:33:03.962+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240422T183301, end_date=20240422T183303
[2024-04-22T18:33:04.017+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:33:04.040+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-01T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:33:04.046+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:34:24.918+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:34:24.924+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T18:34:24.925+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:34:24.938+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-22T18:34:24.941+0000] {standard_task_runner.py:60} INFO - Started process 256114 to run task
[2024-04-22T18:34:24.943+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '184', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpfqv715_l']
[2024-04-22T18:34:24.944+0000] {standard_task_runner.py:88} INFO - Job 184: Subtask read_equipements
[2024-04-22T18:34:24.982+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:34:25.016+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:34:25.040+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-22T18:34:25.041+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:34:25.041+0000] {logging_mixin.py:188} INFO - intervals is ['None', '2024-03-01 00:00:00+00:00']
[2024-04-22T18:34:27.213+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:34:27.330+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240422T183424, end_date=20240422T183427
[2024-04-22T18:34:27.362+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:34:27.385+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-01T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:34:27.393+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T19:36:58.633+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T19:36:58.641+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-22T19:36:58.641+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T19:36:58.655+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-22T19:36:58.659+0000] {standard_task_runner.py:60} INFO - Started process 306768 to run task
[2024-04-22T19:36:58.662+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '193', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpva_2b_t9']
[2024-04-22T19:36:58.662+0000] {standard_task_runner.py:88} INFO - Job 193: Subtask read_equipements
[2024-04-22T19:36:58.692+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T19:36:58.767+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-22T19:36:58.768+0000] {pipeline_eq.py:64} INFO - get date
[2024-04-22T19:36:58.768+0000] {rw.py:44} INFO - start to get objects in minio
[2024-04-22T19:37:00.834+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 69, in read_equipements
    file = MINIO_CLIENT.get_object("extracted", dates_files[0])
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/minio/api.py", line 1244, in get_object
    return self._execute(
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/minio/api.py", line 440, in _execute
    return self._url_open(
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/minio/api.py", line 423, in _url_open
    raise response_error
minio.error.S3Error: S3 operation failed; code: NoSuchKey, message: The specified key does not exist., resource: /extracted/dates_20240301 00:00:00+00:00.json, request_id: 17C8B15137A44D3F, host_id: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8, bucket_name: extracted, object_name: dates_20240301 00:00:00+00:00.json
[2024-04-22T19:37:00.838+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240422T193658, end_date=20240422T193700
[2024-04-22T19:37:00.851+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 193 for task read_equipements (S3 operation failed; code: NoSuchKey, message: The specified key does not exist., resource: /extracted/dates_20240301 00:00:00+00:00.json, request_id: 17C8B15137A44D3F, host_id: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8, bucket_name: extracted, object_name: dates_20240301 00:00:00+00:00.json; 306768)
[2024-04-22T19:37:00.881+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-22T19:37:00.899+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-23T10:29:03.425+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T10:29:03.431+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T10:29:03.431+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-23T10:29:03.442+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-23T10:29:03.446+0000] {standard_task_runner.py:60} INFO - Started process 23937 to run task
[2024-04-23T10:29:03.448+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '200', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpp10f368x']
[2024-04-23T10:29:03.449+0000] {standard_task_runner.py:88} INFO - Job 200: Subtask read_equipements
[2024-04-23T10:29:03.470+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-23T10:29:03.510+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-23T10:29:03.511+0000] {pipeline_eq.py:64} INFO - get date
[2024-04-23T10:29:03.511+0000] {rw.py:43} INFO - start to get objects in minio
[2024-04-23T10:29:08.842+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 69, in read_equipements
    file = MINIO_CLIENT.get_object("extracted", dates_files[0])
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/minio/api.py", line 1244, in get_object
    return self._execute(
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/minio/api.py", line 440, in _execute
    return self._url_open(
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/minio/api.py", line 423, in _url_open
    raise response_error
minio.error.S3Error: S3 operation failed; code: NoSuchKey, message: The specified key does not exist., resource: /extracted/dates_20240301 00:00:00+00:00.json, request_id: 17C8E20033AF9C4F, host_id: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8, bucket_name: extracted, object_name: dates_20240301 00:00:00+00:00.json
[2024-04-23T10:29:08.847+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240423T102903, end_date=20240423T102908
[2024-04-23T10:29:08.858+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 200 for task read_equipements (S3 operation failed; code: NoSuchKey, message: The specified key does not exist., resource: /extracted/dates_20240301 00:00:00+00:00.json, request_id: 17C8E20033AF9C4F, host_id: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8, bucket_name: extracted, object_name: dates_20240301 00:00:00+00:00.json; 23937)
[2024-04-23T10:29:08.877+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-23T10:29:08.897+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-23T10:37:12.175+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T10:37:12.181+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T10:37:12.181+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-23T10:37:12.194+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-23T10:37:12.197+0000] {standard_task_runner.py:60} INFO - Started process 30786 to run task
[2024-04-23T10:37:12.200+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '204', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpgwxks_rl']
[2024-04-23T10:37:12.200+0000] {standard_task_runner.py:88} INFO - Job 204: Subtask read_equipements
[2024-04-23T10:37:12.223+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-23T10:37:12.266+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-23T10:37:12.267+0000] {pipeline_eq.py:64} INFO - get date
[2024-04-23T10:37:12.267+0000] {rw.py:43} INFO - start to get objects in minio
[2024-04-23T10:37:15.235+0000] {pipeline_eq.py:71} INFO - start to get objects in minio
[2024-04-23T10:37:15.236+0000] {logging_mixin.py:188} INFO - intervals is {'start': '2024-03-01 00:00:00+00:00', 'end': '2024-04-23'}
[2024-04-23T10:37:33.810+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 78, in read_equipements
    good_objects = [obj.object_name for obj in objects if obj.object_name.lower().endswith(tuple(extensions)) and obj.last_modified in [intervals["start"].strptime("%Y-%m-%d"), intervals["end"].strptime("%Y-%m-%d")] ]
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 78, in <listcomp>
    good_objects = [obj.object_name for obj in objects if obj.object_name.lower().endswith(tuple(extensions)) and obj.last_modified in [intervals["start"].strptime("%Y-%m-%d"), intervals["end"].strptime("%Y-%m-%d")] ]
AttributeError: 'str' object has no attribute 'strptime'
[2024-04-23T10:37:33.812+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240423T103712, end_date=20240423T103733
[2024-04-23T10:37:33.822+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 204 for task read_equipements ('str' object has no attribute 'strptime'; 30786)
[2024-04-23T10:37:33.841+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-23T10:37:33.851+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-23T10:46:46.523+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T10:46:46.529+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T10:46:46.529+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-23T10:46:46.540+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-23T10:46:46.543+0000] {standard_task_runner.py:60} INFO - Started process 39166 to run task
[2024-04-23T10:46:46.545+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '209', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmp883mp6wu']
[2024-04-23T10:46:46.546+0000] {standard_task_runner.py:88} INFO - Job 209: Subtask read_equipements
[2024-04-23T10:46:46.569+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-23T10:46:46.608+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-23T10:46:46.609+0000] {pipeline_eq.py:64} INFO - get date
[2024-04-23T10:46:46.609+0000] {rw.py:43} INFO - start to get objects in minio
[2024-04-23T10:46:50.008+0000] {pipeline_eq.py:71} INFO - start to get objects in minio
[2024-04-23T10:46:50.009+0000] {logging_mixin.py:188} INFO - intervals is {'start': '2024-03-01 00:00:00+00:00', 'end': '2024-04-23'}
[2024-04-23T10:46:57.786+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 78, in read_equipements
    good_objects = [obj.object_name for obj in objects if obj.object_name.lower().endswith(tuple(extensions)) and obj.last_modified in [ datetime.strptime("%Y-%m-%d", intervals["start"] ), datetime.strptime("%Y-%m-%d", intervals["end"] )] ]
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 78, in <listcomp>
    good_objects = [obj.object_name for obj in objects if obj.object_name.lower().endswith(tuple(extensions)) and obj.last_modified in [ datetime.strptime("%Y-%m-%d", intervals["start"] ), datetime.strptime("%Y-%m-%d", intervals["end"] )] ]
  File "/home/yasmine/.pyenv/versions/3.8.16/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/home/yasmine/.pyenv/versions/3.8.16/lib/python3.8/_strptime.py", line 349, in _strptime
    raise ValueError("time data %r does not match format %r" %
ValueError: time data '%Y-%m-%d' does not match format '2024-03-01 00:00:00+00:00'
[2024-04-23T10:46:57.789+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240423T104646, end_date=20240423T104657
[2024-04-23T10:46:57.798+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 209 for task read_equipements (time data '%Y-%m-%d' does not match format '2024-03-01 00:00:00+00:00'; 39166)
[2024-04-23T10:46:57.817+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-23T10:46:57.839+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-23T11:04:51.419+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T11:04:51.424+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T11:04:51.424+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-23T11:04:51.440+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-23T11:04:51.444+0000] {standard_task_runner.py:60} INFO - Started process 56754 to run task
[2024-04-23T11:04:51.446+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '219', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmprg4dnlpe']
[2024-04-23T11:04:51.447+0000] {standard_task_runner.py:88} INFO - Job 219: Subtask read_equipements
[2024-04-23T11:04:51.494+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-23T11:04:51.579+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-23T11:04:51.581+0000] {pipeline_eq.py:65} INFO - get date
[2024-04-23T11:04:51.581+0000] {rw.py:43} INFO - start to get objects in minio
[2024-04-23T11:04:52.169+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/minio/helpers.py", line 264, in check_non_empty_string
    if not string.strip():
AttributeError: 'datetime.datetime' object has no attribute 'strip'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 70, in read_equipements
    file = MINIO_CLIENT.get_object("intervalles", dates_files[0])
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/minio/api.py", line 1230, in get_object
    check_non_empty_string(object_name)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/minio/helpers.py", line 267, in check_non_empty_string
    raise TypeError() from exc
TypeError
[2024-04-23T11:04:52.172+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240423T110451, end_date=20240423T110452
[2024-04-23T11:04:52.184+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 219 for task read_equipements (; 56754)
[2024-04-23T11:04:52.224+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-23T11:04:52.335+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-23T11:15:50.044+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T11:15:50.050+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T11:15:50.050+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-23T11:15:50.063+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-23T11:15:50.067+0000] {standard_task_runner.py:60} INFO - Started process 68308 to run task
[2024-04-23T11:15:50.069+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '228', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpfyr5vf0o']
[2024-04-23T11:15:50.069+0000] {standard_task_runner.py:88} INFO - Job 228: Subtask read_equipements
[2024-04-23T11:15:50.095+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-23T11:15:50.265+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-23T11:15:50.266+0000] {pipeline_eq.py:65} INFO - get date
[2024-04-23T11:15:50.266+0000] {rw.py:43} INFO - start to get objects in minio
[2024-04-23T11:15:50.266+0000] {rw.py:45} INFO - get objects
[2024-04-23T11:15:50.266+0000] {rw.py:48} INFO - get good objects
[2024-04-23T11:15:53.789+0000] {pipeline_eq.py:72} INFO - start to get objects in minio
[2024-04-23T11:15:53.790+0000] {logging_mixin.py:188} INFO - intervals is {'start': '2024-03-01 00:00:00+00:00', 'end': '2024-04-23'}
[2024-04-23T11:16:03.340+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-23T11:16:03.356+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240423T111550, end_date=20240423T111603
[2024-04-23T11:16:03.388+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-23T11:16:03.415+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-01T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-23T11:16:03.419+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-23T11:17:55.545+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T11:17:55.550+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T11:17:55.550+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-23T11:17:55.561+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-23T11:17:55.564+0000] {standard_task_runner.py:60} INFO - Started process 71141 to run task
[2024-04-23T11:17:55.567+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '233', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmp4hiituro']
[2024-04-23T11:17:55.567+0000] {standard_task_runner.py:88} INFO - Job 233: Subtask read_equipements
[2024-04-23T11:17:55.592+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-23T11:17:55.747+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-23T11:17:55.747+0000] {pipeline_eq.py:65} INFO - get date
[2024-04-23T11:17:55.748+0000] {rw.py:43} INFO - start to get objects in minio
[2024-04-23T11:17:55.748+0000] {rw.py:45} INFO - get objects
[2024-04-23T11:17:55.748+0000] {rw.py:48} INFO - get good objects
[2024-04-23T11:17:58.746+0000] {connectionpool.py:874} WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /intervalles?location=
[2024-04-23T11:18:00.682+0000] {connectionpool.py:874} WARNING - Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /intervalles?location=
[2024-04-23T11:18:02.228+0000] {pipeline_eq.py:72} INFO - start to get objects in minio
[2024-04-23T11:18:02.228+0000] {logging_mixin.py:188} INFO - intervals is {'start': '2024-03-01 00:00:00+00:00', 'end': '2024-04-23'}
[2024-04-23T11:18:17.003+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-23T11:18:17.016+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240423T111755, end_date=20240423T111817
[2024-04-23T11:18:17.049+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-23T11:18:17.079+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-01T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-23T11:18:17.084+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-23T11:23:29.326+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T11:23:29.330+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [queued]>
[2024-04-23T11:23:29.331+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-23T11:23:29.341+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-01 00:00:00+00:00
[2024-04-23T11:23:29.344+0000] {standard_task_runner.py:60} INFO - Started process 79536 to run task
[2024-04-23T11:23:29.346+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-01T00:00:00+00:00', '--job-id', '261', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmprvyg4nio']
[2024-04-23T11:23:29.347+0000] {standard_task_runner.py:88} INFO - Job 261: Subtask read_equipements
[2024-04-23T11:23:29.370+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-01T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-23T11:23:29.513+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-01T00:00:00+00:00'
[2024-04-23T11:23:29.514+0000] {pipeline_eq.py:66} INFO - get date
[2024-04-23T11:23:29.514+0000] {rw.py:43} INFO - start to get objects in minio
[2024-04-23T11:23:29.514+0000] {rw.py:45} INFO - get objects
[2024-04-23T11:23:29.514+0000] {rw.py:48} INFO - get good objects
[2024-04-23T11:23:32.378+0000] {connectionpool.py:874} WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /intervalles?location=
[2024-04-23T11:23:33.432+0000] {logging_mixin.py:188} INFO - {'start': '2024-03-01 00:00:00+00:00', 'end': '2024-04-23'}
[2024-04-23T11:23:33.432+0000] {pipeline_eq.py:74} INFO - start to get objects in minio
[2024-04-23T11:23:33.433+0000] {logging_mixin.py:188} INFO - intervals is {'start': '2024-03-01 00:00:00+00:00', 'end': '2024-04-23'}
[2024-04-23T11:23:44.218+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 81, in read_equipements
    good_objects = [obj.object_name for obj in objects if obj.object_name.lower().endswith(tuple(extensions)) and pd.to_datetime(intervals["start"], "%Y-%m-%d %H:%M:%S")<obj.last_modified <=  pd.to_datetime(intervals["start", "%Y-%m-%d %H:%M:%S"]) ]
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 81, in <listcomp>
    good_objects = [obj.object_name for obj in objects if obj.object_name.lower().endswith(tuple(extensions)) and pd.to_datetime(intervals["start"], "%Y-%m-%d %H:%M:%S")<obj.last_modified <=  pd.to_datetime(intervals["start", "%Y-%m-%d %H:%M:%S"]) ]
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/pandas/core/tools/datetimes.py", line 1084, in to_datetime
    result = convert_listlike(np.array([arg]), format)[0]
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/pandas/core/tools/datetimes.py", line 453, in _convert_listlike_datetimes
    return _array_strptime_with_fallback(arg, name, utc, format, exact, errors)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/pandas/core/tools/datetimes.py", line 484, in _array_strptime_with_fallback
    result, timezones = array_strptime(arg, fmt, exact=exact, errors=errors, utc=utc)
  File "pandas/_libs/tslibs/strptime.pyx", line 191, in pandas._libs.tslibs.strptime.array_strptime
AssertionError
[2024-04-23T11:23:44.221+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240301T000000, start_date=20240423T112329, end_date=20240423T112344
[2024-04-23T11:23:44.232+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 261 for task read_equipements (; 79536)
[2024-04-23T11:23:44.238+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-23T11:23:44.250+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
