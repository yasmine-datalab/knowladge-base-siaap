[2024-04-22T16:56:07.910+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T16:56:07.922+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T16:56:07.923+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T16:56:07.939+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-22T16:56:07.944+0000] {standard_task_runner.py:60} INFO - Started process 165152 to run task
[2024-04-22T16:56:07.948+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpd95_0aqn']
[2024-04-22T16:56:07.949+0000] {standard_task_runner.py:88} INFO - Job 8: Subtask read_equipements
[2024-04-22T16:56:07.992+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T16:56:08.054+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-22T16:56:08.055+0000] {pipeline_eq.py:47} INFO - start to get objects in minio
[2024-04-22T16:56:08.061+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:315 AirflowContextDeprecationWarning: Accessing 'prev_execution_date_success' from the template is deprecated and will be removed in a future version. Please use 'prev_data_interval_start_success' instead.
[2024-04-22T16:56:08.062+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 48, in read_equipements
    intervals =[context['prev_execution_date_success'].date, context['execution_date'].date]
AttributeError: 'NoneType' object has no attribute 'date'
[2024-04-22T16:56:08.067+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240422T165607, end_date=20240422T165608
[2024-04-22T16:56:08.077+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 8 for task read_equipements ('NoneType' object has no attribute 'date'; 165152)
[2024-04-22T16:56:08.120+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-22T16:56:08.134+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T17:10:01.446+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T17:10:01.454+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T17:10:01.455+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T17:10:01.469+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-22T17:10:01.472+0000] {standard_task_runner.py:60} INFO - Started process 179262 to run task
[2024-04-22T17:10:01.476+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '60', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpqi2959p4']
[2024-04-22T17:10:01.477+0000] {standard_task_runner.py:88} INFO - Job 60: Subtask read_equipements
[2024-04-22T17:10:01.517+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T17:10:01.571+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-22T17:10:01.572+0000] {pipeline_eq.py:48} INFO - start to get objects in minio
[2024-04-22T17:10:03.156+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:315 AirflowContextDeprecationWarning: Accessing 'prev_execution_date_success' from the template is deprecated and will be removed in a future version. Please use 'prev_data_interval_start_success' instead.
[2024-04-22T17:10:03.157+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:315 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T17:10:03.158+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 64, in read_equipements
    raise ValueError(f"No files found with good extensions {extensions}")
ValueError: No files found with good extensions ['.json']
[2024-04-22T17:10:03.162+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240422T171001, end_date=20240422T171003
[2024-04-22T17:10:03.170+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 60 for task read_equipements (No files found with good extensions ['.json']; 179262)
[2024-04-22T17:10:03.212+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-22T17:10:03.229+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T17:12:40.329+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T17:12:40.335+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T17:12:40.336+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T17:12:40.349+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-22T17:12:40.353+0000] {standard_task_runner.py:60} INFO - Started process 182052 to run task
[2024-04-22T17:12:40.356+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '63', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpfep31apu']
[2024-04-22T17:12:40.356+0000] {standard_task_runner.py:88} INFO - Job 63: Subtask read_equipements
[2024-04-22T17:12:40.389+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T17:12:40.441+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-22T17:12:40.442+0000] {pipeline_eq.py:48} INFO - start to get objects in minio
[2024-04-22T17:12:42.410+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:315 AirflowContextDeprecationWarning: Accessing 'prev_execution_date_success' from the template is deprecated and will be removed in a future version. Please use 'prev_data_interval_start_success' instead.
[2024-04-22T17:12:42.410+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 60, in read_equipements
    good_objects = [obj.object_name for obj in objects if obj.object_name.lower().endswith(tuple(extensions)) and obj.last_modified.date in [intervals[0].date, intervals[1].date] ]
  File "/home/yasmine/Documents/FREELANCE/knowledge-base-siaap/dags/kbs/custom_dags/pipeline_eq.py", line 60, in <listcomp>
    good_objects = [obj.object_name for obj in objects if obj.object_name.lower().endswith(tuple(extensions)) and obj.last_modified.date in [intervals[0].date, intervals[1].date] ]
AttributeError: 'NoneType' object has no attribute 'date'
[2024-04-22T17:12:42.414+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240422T171240, end_date=20240422T171242
[2024-04-22T17:12:42.424+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 63 for task read_equipements ('NoneType' object has no attribute 'date'; 182052)
[2024-04-22T17:12:42.453+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-22T17:12:42.466+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:16:56.348+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:16:56.357+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:16:56.357+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:16:56.373+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-22T18:16:56.375+0000] {standard_task_runner.py:60} INFO - Started process 234829 to run task
[2024-04-22T18:16:56.378+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '96', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpddmx6ob6']
[2024-04-22T18:16:56.378+0000] {standard_task_runner.py:88} INFO - Job 96: Subtask read_equipements
[2024-04-22T18:16:56.414+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:16:56.453+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:16:56.475+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-22T18:16:56.476+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:16:56.476+0000] {logging_mixin.py:188} INFO - intervals is ['2024-03-02 00:00:00+00:00', '2024-03-02 00:00:00+00:00']
[2024-04-22T18:16:58.513+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:16:58.533+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240422T181656, end_date=20240422T181658
[2024-04-22T18:16:58.557+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:16:58.581+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-02T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:16:58.588+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:25:15.870+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:25:15.878+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:25:15.878+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:25:15.893+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-22T18:25:15.897+0000] {standard_task_runner.py:60} INFO - Started process 245268 to run task
[2024-04-22T18:25:15.901+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '149', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmp41i7r7i3']
[2024-04-22T18:25:15.901+0000] {standard_task_runner.py:88} INFO - Job 149: Subtask read_equipements
[2024-04-22T18:25:15.930+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:25:15.969+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:25:15.991+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-22T18:25:15.992+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:25:15.992+0000] {logging_mixin.py:188} INFO - intervals is ['2024-03-02 00:00:00+00:00', '2024-03-02 00:00:00+00:00']
[2024-04-22T18:25:18.031+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:25:18.048+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240422T182515, end_date=20240422T182518
[2024-04-22T18:25:18.079+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:25:18.107+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-02T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:25:18.112+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:27:10.401+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:27:10.413+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:27:10.413+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:27:10.427+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-22T18:27:10.431+0000] {standard_task_runner.py:60} INFO - Started process 247632 to run task
[2024-04-22T18:27:10.433+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '161', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpb3d8ix_q']
[2024-04-22T18:27:10.434+0000] {standard_task_runner.py:88} INFO - Job 161: Subtask read_equipements
[2024-04-22T18:27:10.459+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:27:10.500+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:27:10.536+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-22T18:27:10.537+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:27:10.538+0000] {logging_mixin.py:188} INFO - intervals is ['2024-03-02 00:00:00+00:00', '2024-03-02 00:00:00+00:00']
[2024-04-22T18:27:12.719+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:27:12.735+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240422T182710, end_date=20240422T182712
[2024-04-22T18:27:12.771+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:27:12.796+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-02T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:27:12.801+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:31:35.124+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:31:35.132+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:31:35.133+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:31:35.146+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-22T18:31:35.150+0000] {standard_task_runner.py:60} INFO - Started process 252657 to run task
[2024-04-22T18:31:35.153+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '176', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmp7hx1bnbj']
[2024-04-22T18:31:35.153+0000] {standard_task_runner.py:88} INFO - Job 176: Subtask read_equipements
[2024-04-22T18:31:35.190+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:31:35.237+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:31:35.267+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-22T18:31:35.268+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:31:35.268+0000] {logging_mixin.py:188} INFO - intervals is ['2024-03-02 00:00:00+00:00', '2024-03-02 00:00:00+00:00']
[2024-04-22T18:31:37.032+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:31:37.052+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240422T183135, end_date=20240422T183137
[2024-04-22T18:31:37.089+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:31:37.127+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-02T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:31:37.137+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:33:06.130+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:33:06.138+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:33:06.138+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:33:06.151+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-22T18:33:06.154+0000] {standard_task_runner.py:60} INFO - Started process 254467 to run task
[2024-04-22T18:33:06.156+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '180', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmp2b_sdew2']
[2024-04-22T18:33:06.157+0000] {standard_task_runner.py:88} INFO - Job 180: Subtask read_equipements
[2024-04-22T18:33:06.182+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:33:06.217+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:33:06.236+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-22T18:33:06.237+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:33:06.237+0000] {logging_mixin.py:188} INFO - intervals is ['2024-03-02 00:00:00+00:00', '2024-03-02 00:00:00+00:00']
[2024-04-22T18:33:08.455+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:33:08.472+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240422T183306, end_date=20240422T183308
[2024-04-22T18:33:08.494+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:33:08.522+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-02T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:33:08.528+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-22T18:34:29.885+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:34:29.894+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-22T18:34:29.894+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-22T18:34:29.908+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-22T18:34:29.911+0000] {standard_task_runner.py:60} INFO - Started process 256240 to run task
[2024-04-22T18:34:29.913+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '185', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmp5z_158fq']
[2024-04-22T18:34:29.914+0000] {standard_task_runner.py:88} INFO - Job 185: Subtask read_equipements
[2024-04-22T18:34:29.944+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-22T18:34:29.992+0000] {logging_mixin.py:188} WARNING - /home/yasmine/.local/share/virtualenvs/knowledge-base-siaap-WvBZnYs-/lib/python3.8/site-packages/airflow/utils/context.py:207 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2024-04-22T18:34:30.023+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-22T18:34:30.025+0000] {pipeline_eq.py:53} INFO - start to get objects in minio
[2024-04-22T18:34:30.025+0000] {logging_mixin.py:188} INFO - intervals is ['2024-03-02 00:00:00+00:00', '2024-03-02 00:00:00+00:00']
[2024-04-22T18:34:32.259+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-22T18:34:32.276+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240422T183429, end_date=20240422T183432
[2024-04-22T18:34:32.291+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-22T18:34:32.320+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-02T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-22T18:34:32.325+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-23T11:16:12.300+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-23T11:16:12.305+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-23T11:16:12.305+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-23T11:16:12.316+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-23T11:16:12.319+0000] {standard_task_runner.py:60} INFO - Started process 68859 to run task
[2024-04-23T11:16:12.321+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '230', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmp9ypy94i3']
[2024-04-23T11:16:12.322+0000] {standard_task_runner.py:88} INFO - Job 230: Subtask read_equipements
[2024-04-23T11:16:12.342+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-23T11:16:12.492+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-23T11:16:12.493+0000] {pipeline_eq.py:65} INFO - get date
[2024-04-23T11:16:12.493+0000] {rw.py:43} INFO - start to get objects in minio
[2024-04-23T11:16:12.493+0000] {rw.py:45} INFO - get objects
[2024-04-23T11:16:12.493+0000] {rw.py:48} INFO - get good objects
[2024-04-23T11:16:16.721+0000] {pipeline_eq.py:72} INFO - start to get objects in minio
[2024-04-23T11:16:16.721+0000] {logging_mixin.py:188} INFO - intervals is {'start': '2024-03-02 00:00:00+00:00', 'end': '2024-04-23'}
[2024-04-23T11:16:37.766+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-23T11:16:37.781+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240423T111612, end_date=20240423T111637
[2024-04-23T11:16:37.799+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-23T11:16:37.820+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-02T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-23T11:16:37.825+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-23T11:18:24.363+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-23T11:18:24.369+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [queued]>
[2024-04-23T11:18:24.370+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-04-23T11:18:24.382+0000] {taskinstance.py:2217} INFO - Executing <Task(_PythonDecoratedOperator): read_equipements> on 2024-03-02 00:00:00+00:00
[2024-04-23T11:18:24.386+0000] {standard_task_runner.py:60} INFO - Started process 71907 to run task
[2024-04-23T11:18:24.388+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'pipeline_eq', 'read_equipements', 'scheduled__2024-03-02T00:00:00+00:00', '--job-id', '235', '--raw', '--subdir', 'DAGS_FOLDER/kbs/custom_dags/pipeline_eq.py', '--cfg-path', '/tmp/tmpwa1bhew5']
[2024-04-23T11:18:24.389+0000] {standard_task_runner.py:88} INFO - Job 235: Subtask read_equipements
[2024-04-23T11:18:24.409+0000] {task_command.py:423} INFO - Running <TaskInstance: pipeline_eq.read_equipements scheduled__2024-03-02T00:00:00+00:00 [running]> on host datalab-yasmine
[2024-04-23T11:18:24.574+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='pipeline_eq' AIRFLOW_CTX_TASK_ID='read_equipements' AIRFLOW_CTX_EXECUTION_DATE='2024-03-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-02T00:00:00+00:00'
[2024-04-23T11:18:24.575+0000] {pipeline_eq.py:65} INFO - get date
[2024-04-23T11:18:24.575+0000] {rw.py:43} INFO - start to get objects in minio
[2024-04-23T11:18:24.575+0000] {rw.py:45} INFO - get objects
[2024-04-23T11:18:24.575+0000] {rw.py:48} INFO - get good objects
[2024-04-23T11:18:29.512+0000] {pipeline_eq.py:72} INFO - start to get objects in minio
[2024-04-23T11:18:29.513+0000] {logging_mixin.py:188} INFO - intervals is {'start': '2024-03-02 00:00:00+00:00', 'end': '2024-04-23'}
[2024-04-23T11:18:52.352+0000] {python.py:202} INFO - Done. Returned value was: []
[2024-04-23T11:18:52.368+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=pipeline_eq, task_id=read_equipements, execution_date=20240302T000000, start_date=20240423T111824, end_date=20240423T111852
[2024-04-23T11:18:52.414+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-04-23T11:18:52.431+0000] {abstractoperator.py:567} INFO - Marking <TaskInstance: pipeline_eq.push_in_redis scheduled__2024-03-02T00:00:00+00:00 [None]> as SKIPPED since the map has 0 values to expand
[2024-04-23T11:18:52.436+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
